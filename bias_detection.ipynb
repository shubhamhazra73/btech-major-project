{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "41qzBa-IG3gt"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzrfeMfpwp4"
      },
      "source": [
        "#METHOD1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asyfx-PApxF1",
        "outputId": "1897f4f0-4169-447b-d8fa-c898eb2ac35f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['experienced', 'rape', 'sexualassault', 'plenty', 'support', 'available', 'free', 'helpline', 'also', 'isva', 'service', 'vscumbria', 'counselling', 'safetynetnorth', 'birchalltrust', 'report', 'cumbriapolice', 'nwsvweek', 'mentoo', 'lgbtq', 'cumbria', 'itsnotok']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load the document\n",
        "filename = 'placelist1000.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyMTdXakrx4x",
        "outputId": "3f1f066a-0ecf-4791-8ba7-0a4ec08850d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9564\n",
            "[('genderpaygap', 411), ('breastfeeding', 300), ('women', 272), ('men', 241), ('misandry', 207), ('menaretrash', 207), ('hatemen', 191), ('lgbtq', 165), ('pay', 127), ('womenarebaddrivers', 126), ('gender', 124), ('de', 111), ('womenarestupid', 95), ('equalityofsexes', 87), ('gap', 82), ('le', 77), ('hatewomen', 73), ('les', 67), ('que', 65), ('la', 62), ('new', 61), ('equality', 61), ('man', 60), ('pas', 60), ('die', 58), ('like', 57), ('one', 56), ('tu', 54), ('work', 53), ('et', 53), ('dont', 53), ('feminism', 52), ('der', 52), ('baby', 51), ('support', 51), ('des', 50), ('im', 49), ('cest', 48), ('would', 47), ('frauen', 47), ('forcedbirth', 47), ('woman', 45), ('get', 45), ('un', 45), ('also', 44), ('help', 41), ('und', 40), ('read', 39), ('time', 39), ('genderequality', 39)]\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('drive/MyDrive/pos', vocab)\n",
        "process_docs('drive/MyDrive/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w58VmXt6uzkV",
        "outputId": "63a58ead-a25f-4cf7-f6ff-7459c940b54b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3030\n"
          ]
        }
      ],
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Y4TEL0uz6k"
      },
      "outputs": [],
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83yKqMnovOrz"
      },
      "outputs": [],
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnfab2zzvQo5"
      },
      "outputs": [],
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4EyC6iJvSg9"
      },
      "outputs": [],
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhpkGdpVvUId",
        "outputId": "f821462e-2948-48b4-8222-7894d9be3462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "892 892\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "positive_lines = process_docs('drive/MyDrive/pos', vocab)\n",
        "negative_lines = process_docs('drive/MyDrive/neg', vocab)\n",
        "# summarize what we have\n",
        "print(len(positive_lines), len(negative_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fq0ox15vhap",
        "outputId": "3276871b-5e73-4de4-e177-abd64528975a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1784, 3031)\n",
            "(222, 3031)\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "positive_lines = process_docs('drive/MyDrive/pos', vocab, True)\n",
        "negative_lines = process_docs('drive/MyDrive/neg', vocab, True)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "docs = negative_lines + positive_lines\n",
        "tokenizer.fit_on_texts(docs)\n",
        "\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "print(Xtrain.shape)\n",
        "\n",
        "# load all test reviews\n",
        "positive_lines = process_docs('drive/MyDrive/pos', vocab, False)\n",
        "negative_lines = process_docs('drive/MyDrive/neg', vocab, False)\n",
        "docs = negative_lines + positive_lines\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "print(Xtest.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMuRFLEDvtrm",
        "outputId": "3e62d6d0-4daa-4707-f138-cfb1ee706960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "56/56 - 1s - loss: 0.6680 - accuracy: 0.8778 - 620ms/epoch - 11ms/step\n",
            "Epoch 2/50\n",
            "56/56 - 0s - loss: 0.5644 - accuracy: 0.9849 - 161ms/epoch - 3ms/step\n",
            "Epoch 3/50\n",
            "56/56 - 0s - loss: 0.4220 - accuracy: 0.9871 - 172ms/epoch - 3ms/step\n",
            "Epoch 4/50\n",
            "56/56 - 0s - loss: 0.2977 - accuracy: 0.9893 - 180ms/epoch - 3ms/step\n",
            "Epoch 5/50\n",
            "56/56 - 0s - loss: 0.2111 - accuracy: 0.9922 - 181ms/epoch - 3ms/step\n",
            "Epoch 6/50\n",
            "56/56 - 0s - loss: 0.1549 - accuracy: 0.9916 - 161ms/epoch - 3ms/step\n",
            "Epoch 7/50\n",
            "56/56 - 0s - loss: 0.1179 - accuracy: 0.9938 - 174ms/epoch - 3ms/step\n",
            "Epoch 8/50\n",
            "56/56 - 0s - loss: 0.0924 - accuracy: 0.9944 - 160ms/epoch - 3ms/step\n",
            "Epoch 9/50\n",
            "56/56 - 0s - loss: 0.0746 - accuracy: 0.9950 - 174ms/epoch - 3ms/step\n",
            "Epoch 10/50\n",
            "56/56 - 0s - loss: 0.0613 - accuracy: 0.9955 - 190ms/epoch - 3ms/step\n",
            "Epoch 11/50\n",
            "56/56 - 0s - loss: 0.0513 - accuracy: 0.9966 - 202ms/epoch - 4ms/step\n",
            "Epoch 12/50\n",
            "56/56 - 0s - loss: 0.0435 - accuracy: 0.9966 - 171ms/epoch - 3ms/step\n",
            "Epoch 13/50\n",
            "56/56 - 0s - loss: 0.0373 - accuracy: 0.9978 - 174ms/epoch - 3ms/step\n",
            "Epoch 14/50\n",
            "56/56 - 0s - loss: 0.0324 - accuracy: 0.9983 - 168ms/epoch - 3ms/step\n",
            "Epoch 15/50\n",
            "56/56 - 0s - loss: 0.0284 - accuracy: 0.9983 - 166ms/epoch - 3ms/step\n",
            "Epoch 16/50\n",
            "56/56 - 0s - loss: 0.0252 - accuracy: 0.9983 - 191ms/epoch - 3ms/step\n",
            "Epoch 17/50\n",
            "56/56 - 0s - loss: 0.0224 - accuracy: 0.9983 - 168ms/epoch - 3ms/step\n",
            "Epoch 18/50\n",
            "56/56 - 0s - loss: 0.0202 - accuracy: 0.9983 - 162ms/epoch - 3ms/step\n",
            "Epoch 19/50\n",
            "56/56 - 0s - loss: 0.0183 - accuracy: 0.9983 - 167ms/epoch - 3ms/step\n",
            "Epoch 20/50\n",
            "56/56 - 0s - loss: 0.0167 - accuracy: 0.9983 - 180ms/epoch - 3ms/step\n",
            "Epoch 21/50\n",
            "56/56 - 0s - loss: 0.0153 - accuracy: 0.9983 - 169ms/epoch - 3ms/step\n",
            "Epoch 22/50\n",
            "56/56 - 0s - loss: 0.0141 - accuracy: 0.9983 - 197ms/epoch - 4ms/step\n",
            "Epoch 23/50\n",
            "56/56 - 0s - loss: 0.0130 - accuracy: 0.9983 - 161ms/epoch - 3ms/step\n",
            "Epoch 24/50\n",
            "56/56 - 0s - loss: 0.0121 - accuracy: 0.9983 - 171ms/epoch - 3ms/step\n",
            "Epoch 25/50\n",
            "56/56 - 0s - loss: 0.0113 - accuracy: 0.9983 - 168ms/epoch - 3ms/step\n",
            "Epoch 26/50\n",
            "56/56 - 0s - loss: 0.0107 - accuracy: 0.9983 - 160ms/epoch - 3ms/step\n",
            "Epoch 27/50\n",
            "56/56 - 0s - loss: 0.0100 - accuracy: 0.9983 - 168ms/epoch - 3ms/step\n",
            "Epoch 28/50\n",
            "56/56 - 0s - loss: 0.0096 - accuracy: 0.9983 - 175ms/epoch - 3ms/step\n",
            "Epoch 29/50\n",
            "56/56 - 0s - loss: 0.0091 - accuracy: 0.9983 - 160ms/epoch - 3ms/step\n",
            "Epoch 30/50\n",
            "56/56 - 0s - loss: 0.0085 - accuracy: 0.9983 - 160ms/epoch - 3ms/step\n",
            "Epoch 31/50\n",
            "56/56 - 0s - loss: 0.0081 - accuracy: 0.9983 - 161ms/epoch - 3ms/step\n",
            "Epoch 32/50\n",
            "56/56 - 0s - loss: 0.0078 - accuracy: 0.9983 - 170ms/epoch - 3ms/step\n",
            "Epoch 33/50\n",
            "56/56 - 0s - loss: 0.0076 - accuracy: 0.9983 - 174ms/epoch - 3ms/step\n",
            "Epoch 34/50\n",
            "56/56 - 0s - loss: 0.0071 - accuracy: 0.9983 - 163ms/epoch - 3ms/step\n",
            "Epoch 35/50\n",
            "56/56 - 0s - loss: 0.0070 - accuracy: 0.9983 - 165ms/epoch - 3ms/step\n",
            "Epoch 36/50\n",
            "56/56 - 0s - loss: 0.0067 - accuracy: 0.9983 - 171ms/epoch - 3ms/step\n",
            "Epoch 37/50\n",
            "56/56 - 0s - loss: 0.0066 - accuracy: 0.9983 - 172ms/epoch - 3ms/step\n",
            "Epoch 38/50\n",
            "56/56 - 0s - loss: 0.0063 - accuracy: 0.9983 - 160ms/epoch - 3ms/step\n",
            "Epoch 39/50\n",
            "56/56 - 0s - loss: 0.0063 - accuracy: 0.9983 - 191ms/epoch - 3ms/step\n",
            "Epoch 40/50\n",
            "56/56 - 0s - loss: 0.0061 - accuracy: 0.9983 - 169ms/epoch - 3ms/step\n",
            "Epoch 41/50\n",
            "56/56 - 0s - loss: 0.0058 - accuracy: 0.9983 - 167ms/epoch - 3ms/step\n",
            "Epoch 42/50\n",
            "56/56 - 0s - loss: 0.0057 - accuracy: 0.9983 - 168ms/epoch - 3ms/step\n",
            "Epoch 43/50\n",
            "56/56 - 0s - loss: 0.0055 - accuracy: 0.9983 - 164ms/epoch - 3ms/step\n",
            "Epoch 44/50\n",
            "56/56 - 0s - loss: 0.0057 - accuracy: 0.9972 - 165ms/epoch - 3ms/step\n",
            "Epoch 45/50\n",
            "56/56 - 0s - loss: 0.0053 - accuracy: 0.9983 - 184ms/epoch - 3ms/step\n",
            "Epoch 46/50\n",
            "56/56 - 0s - loss: 0.0051 - accuracy: 0.9983 - 171ms/epoch - 3ms/step\n",
            "Epoch 47/50\n",
            "56/56 - 0s - loss: 0.0053 - accuracy: 0.9983 - 178ms/epoch - 3ms/step\n",
            "Epoch 48/50\n",
            "56/56 - 0s - loss: 0.0051 - accuracy: 0.9983 - 164ms/epoch - 3ms/step\n",
            "Epoch 49/50\n",
            "56/56 - 0s - loss: 0.0050 - accuracy: 0.9983 - 162ms/epoch - 3ms/step\n",
            "Epoch 50/50\n",
            "56/56 - 0s - loss: 0.0050 - accuracy: 0.9983 - 169ms/epoch - 3ms/step\n",
            "Test Accuracy: 98.198199\n"
          ]
        }
      ],
      "source": [
        "from numpy import array\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('placelist9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "positive_lines = process_docs('drive/MyDrive/pos', vocab, True)\n",
        "negative_lines = process_docs('drive/MyDrive/neg', vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "docs = negative_lines + positive_lines\n",
        "tokenizer.fit_on_texts(docs)\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "ytrain = array([0 for _ in range(892)] + [1 for _ in range(892)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_lines = process_docs('drive/MyDrive/pos', vocab, False)\n",
        "negative_lines = process_docs('drive/MyDrive/neg', vocab, False)\n",
        "docs = negative_lines + positive_lines\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "ytest = array([0 for _ in range(111)] + [1 for _ in range(111)])\n",
        "\n",
        "n_words = Xtest.shape[1]\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}